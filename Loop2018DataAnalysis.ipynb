{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues with MADE dataset:\n",
    "FTIR\n",
    "* Dodgy sample numbers\n",
    "* 2 Hit confidence columns\n",
    "* 2 substance detected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "def fix_sample_number(x):\n",
    "    \"\"\"Make sure all samples numbers are of form: AXXX (where A is one of A, F, W and X is a digit)\"\"\"\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return x # leave NaN's alone    \n",
    "    try:\n",
    "        sn = int(x)\n",
    "        sn = 'F{:04d}'.format(int(x))\n",
    "    except ValueError:\n",
    "        # Assume string so make sure it's of the right format\n",
    "        sn = str(x).capitalize()\n",
    "    if sn[0] not in ['A', 'F', 'W']:\n",
    "        print(\"!!! Bad ID %s\" % sn)\n",
    "    return sn\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ftir_csv = 'MADE/FTIR Analysis Data Recording Form.csv'\n",
    "catalog_csv = 'MADE/Sample Cataloguing Form.csv'\n",
    "reagent_csv = 'MADE/Reagent Outcomes.csv'\n",
    "hr_csv = 'MADE/MADE MAST Intervention Questionnaire.csv'\n",
    "\n",
    "date_cols = ['Timestamp']\n",
    "df_ftir = pd.read_csv(ftir_csv, engine=\"python\", parse_dates=date_cols)\n",
    "df_catalog = pd.read_csv(catalog_csv, engine=\"python\", parse_dates=date_cols)\n",
    "df_reagent = pd.read_csv(reagent_csv, engine=\"python\", parse_dates=date_cols)\n",
    "df_hr = pd.read_csv(hr_csv, engine=\"python\", parse_dates=date_cols)\n",
    "\n",
    "mla_excel = 'MADE/MADE - Loop 2018 event results sheet_.xlsx'\n",
    "df_mla = pd.read_excel(mla_excel, sheetname='MLA', header=1)\n",
    "\n",
    "# Sort out column names\n",
    "df_reagent.rename(columns={'Sample Code':'Sample Number', 'Substance(s) detected' : 'Reagent Result'}, inplace=True)\n",
    "df_hr.rename(columns={'Sample Number:':'Sample Number'}, inplace=True)\n",
    "df_mla.rename(columns={'Sample Num':'Sample Number'}, inplace=True)\n",
    "\n",
    "# Make all sample numbers a 4-digit code starting with F\n",
    "df_ftir['Sample Number'] = df_ftir['Sample Number'].apply(fix_sample_number)\n",
    "df_catalog['Sample Number'] = df_catalog['Sample Number'].apply(fix_sample_number)\n",
    "df_reagent['Sample Number'] = df_reagent['Sample Number'].apply(fix_sample_number)\n",
    "df_hr['Sample Number'] = df_hr['Sample Number'].apply(fix_sample_number)\n",
    "df_mla['Sample Number'] = df_mla['Sample Number'].apply(fix_sample_number)\n",
    "\n",
    "# Prune down MLA to valid sample numbers\n",
    "df_mla = df_mla[df_mla['Sample Number'].notnull()]\n",
    "\n",
    "DataFrames = namedtuple('DataFrames', ['catalog', 'ftir', 'reagent','mla', 'hr'])\n",
    "dfs = DataFrames(\n",
    "    catalog=df_catalog,\n",
    "    ftir=df_ftir,\n",
    "    reagent=df_reagent,\n",
    "    mla=df_mla,\n",
    "    hr=df_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Sample Number for catalog\n",
      "!!! Bad ID R0876\n",
      "Fixing Sample Number for ftir\n",
      "!!! Bad ID Z1000\n",
      "!!! Bad ID B0076\n",
      "Fixing Sample Number for reagent\n",
      "!!! Bad ID Z1000\n",
      "!!! Bad ID Yellow > green\n",
      "Fixing Sample Number for mla\n",
      "Fixing Sample Number for hr\n",
      "!!! Bad ID G0037\n",
      "!!! Bad ID G0242\n",
      "!!! Bad ID G0153\n",
      "!!! Bad ID G0024\n",
      "!!! Bad ID G0652\n",
      "!!! Bad ID G0999\n",
      "!!! Bad ID G0877\n",
      "!!! Bad ID G0878\n",
      "!!! Bad ID G0811\n",
      "!!! Bad ID G1441\n",
      "!!! Bad ID G1216\n",
      "!!! Bad ID G1228\n",
      "!!! Bad ID G1229\n",
      "!!! Bad ID G1398\n",
      "!!! Bad ID G1284\n",
      "!!! Bad ID G1572\n",
      "!!! Bad ID G0875\n",
      "!!! Bad ID G1833\n",
      "!!! Bad ID G1703\n",
      "!!! Bad ID G1860\n",
      "!!! Bad ID G1859\n",
      "!!! Bad ID G1699\n",
      "!!! Bad ID G1686\n",
      "!!! Bad ID G1312\n",
      "!!! Bad ID G0420\n",
      "!!! Bad ID G0983\n",
      "!!! Bad ID G0981\n"
     ]
    }
   ],
   "source": [
    "def gsheets_service():\n",
    "    from googleapiclient.discovery import build\n",
    "    from httplib2 import Http\n",
    "    from oauth2client import file, client, tools\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    CREDS_FILE = '/opt/random/MADE/JensDataExportJupyter_client_secret.json'\n",
    "    SCOPES = 'https://www.googleapis.com/auth/spreadsheets.readonly'\n",
    "    store = file.Storage('token.json')\n",
    "    creds = store.get()\n",
    "    if not creds or creds.invalid:\n",
    "        import argparse\n",
    "        flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args([])\n",
    "        flow = client.flow_from_clientsecrets(CREDS_FILE, SCOPES)\n",
    "        creds = tools.run_flow(flow, store, flags)\n",
    "    service = build('sheets', 'v4', http=creds.authorize(Http()))\n",
    "    return service\n",
    "\n",
    "def get_df(service, SPREADSHEET_ID, SS_RANGE, mla=False):\n",
    "    # Call the Sheets API\n",
    "    result = service.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID,\n",
    "                                                range=SS_RANGE).execute()\n",
    "    values = result.get('values', [])\n",
    "    if not values:\n",
    "        print('*** No data found ***')\n",
    "        return None\n",
    "\n",
    "    # mla has irrelevant stuff in columns 1 and 3 and sample numbers in first column\n",
    "    if mla:\n",
    "        values.pop(0)\n",
    "        values.pop(1)\n",
    "        def not_blank(row):\n",
    "            return len(row[0]) > 0       \n",
    "    else:\n",
    "        def not_blank(row):\n",
    "            return sum(map(len, row[:6])) > 0\n",
    "\n",
    "    rows = filter(not_blank, values)\n",
    "    if not rows:\n",
    "        print('*** No data found after pruning rows! ***')\n",
    "        return None\n",
    "    return pd.DataFrame(rows[1:], columns=rows[0])\n",
    "\n",
    "def canonicalise_df(df, source=None):\n",
    "    from pandas._libs.tslib import OutOfBoundsDatetime\n",
    "    \"\"\"Initial cleaning of all dataframes\"\"\"\n",
    "    df.rename(columns={'Sample Code':'Sample Number',\n",
    "                       'Sample Number:':'Sample Number',\n",
    "                       'Sample Num':'Sample Number',\n",
    "                       'Sample Number i.e F0XXX' : 'Sample Number'\n",
    "                      }, inplace=True)\n",
    "    def fix_timestamp(x):\n",
    "        \"\"\"Horrror to fix mixed date formats\"\"\"\n",
    "        try:\n",
    "            x = pd.to_datetime(x)\n",
    "        except OutOfBoundsDatetime:\n",
    "            try:\n",
    "                # '12/08/2018 12:26:15'\n",
    "                x = pd.to_datetime(x, format='%d/%m/%Y %H:%M:%S')\n",
    "            except ValueError:\n",
    "                #'Thu 9/08 - 12:19'\n",
    "                x = pd.to_datetime(x, format='%a %m/%y - %H:%M')\n",
    "        return x\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df.loc[:, 'Timestamp'] = df['Timestamp'].map(fix_timestamp)\n",
    "    if source:\n",
    "        print(\"Fixing Sample Number for %s\" % source)\n",
    "    df.loc[:, 'Sample Number'] = df['Sample Number'].apply(fix_sample_number)\n",
    "    df = df[df['Sample Number'].notnull()]\n",
    "    df.sort_values(['Sample Number'], ascending=True, inplace=True)\n",
    "    return df\n",
    "    \n",
    "# The ID and range of a sample spreadsheet.\n",
    "BOOMTOWN2018_SPREADSHEET_ID = '1RiA-FwG_954Ger2VPsOSA3JLh-7sEoTYr40eVS0mp24'\n",
    "MADE2018_SPREADSHEET_ID = '1daXdyL6uL8qnMsEsP0RLZE9nDzt6J7Zr1ygQdguvi-E'\n",
    "SPREADSHEET_ID = BOOMTOWN2018_SPREADSHEET_ID\n",
    "CATALOG_RANGE = 'Catalog!A:R'\n",
    "FTIR_RANGE = 'FTIR!A:X'\n",
    "REAGENT_RANGE = 'Reagent!A:W'\n",
    "MLA_RANGE = 'MLA!A:R'\n",
    "HR_RANGE = 'Interventions!A:BJ'\n",
    "\n",
    "service = gsheets_service()\n",
    "\n",
    "df_catalog = get_df(service, SPREADSHEET_ID, CATALOG_RANGE)\n",
    "df_catalog = canonicalise_df(df_catalog, source='catalog')\n",
    "df_ftir = get_df(service, SPREADSHEET_ID, FTIR_RANGE)\n",
    "df_ftir = canonicalise_df(df_ftir, source='ftir')\n",
    "df_reagent = get_df(service, SPREADSHEET_ID, REAGENT_RANGE)\n",
    "df_reagent = canonicalise_df(df_reagent, source='reagent')\n",
    "df_mla = get_df(service, SPREADSHEET_ID, MLA_RANGE, mla=True)\n",
    "df_mla = canonicalise_df(df_mla, source='mla')\n",
    "df_hr = get_df(service, SPREADSHEET_ID, HR_RANGE)\n",
    "df_hr = canonicalise_df(df_hr, source='hr')\n",
    "\n",
    "DataFrames = namedtuple('DataFrames', ['catalog', 'ftir', 'reagent','mla', 'hr'])\n",
    "dfs = DataFrames(\n",
    "    catalog=df_catalog,\n",
    "    ftir=df_ftir,\n",
    "    reagent=df_reagent,\n",
    "    mla=df_mla,\n",
    "    hr=df_hr)\n",
    "\n",
    "import pickle\n",
    "with open('foo.pkl','w') as w:\n",
    "    pickle.dump(dfs, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Duplicated catalog sample numbers ###\n",
      "### Duplicated FTIR sample numbers ###\n",
      "### Duplicated catalog sample numbers ###\n",
      "### Duplicated HR sample numbers ###\n",
      "Please fix duplicated values\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# with open('foo.pkl') as f:\n",
    "#     dfs = pickle.load(f)\n",
    "\n",
    "# Check for duplicates\n",
    "if dfs.catalog['Sample Number'].duplicated().any():\n",
    "    print(\"### Duplicated catalog sample numbers ###\")\n",
    "    dfs.catalog[dfs.catalog['Sample Number'].duplicated(keep=False)].to_csv('catalog_duplicates.csv')\n",
    "if dfs.ftir['Sample Number'].duplicated().any():\n",
    "    print(\"### Duplicated FTIR sample numbers ###\")\n",
    "    dfs.ftir[dfs.ftir['Sample Number'].duplicated(keep=False)].to_csv('ftir_duplicates.csv')\n",
    "if dfs.reagent['Sample Number'].duplicated().any():\n",
    "    print(\"### Duplicated catalog sample numbers ###\")    \n",
    "    dfs.reagent[dfs.reagent['Sample Number'].duplicated(keep=False)].to_csv('reagent_duplicates.csv')\n",
    "if dfs.hr['Sample Number'].duplicated().any():\n",
    "    print(\"### Duplicated HR sample numbers ###\")\n",
    "    dfs.hr[dfs.hr['Sample Number'].duplicated(keep=False)].to_csv('hr_duplicates.csv', encoding = 'utf-8')\n",
    "if dfs.mla['Sample Number'].duplicated().any():\n",
    "    print(\"### Duplicated MLA sample numbers ###\")\n",
    "    dfs.mla[dfs.mla['Sample Number'].duplicated(keep=False)].to_csv('mla_duplicates.csv')\n",
    "    \n",
    "if dfs.catalog['Sample Number'].duplicated().any() or \\\n",
    "    dfs.ftir['Sample Number'].duplicated().any() or \\\n",
    "    dfs.reagent['Sample Number'].duplicated().any() or \\\n",
    "    dfs.hr['Sample Number'].duplicated().any() or \\\n",
    "    dfs.mla['Sample Number'].duplicated().any():\n",
    "\n",
    "    outs = 'Please fix duplicated values'\n",
    "    print(outs)\n",
    "#     raise RuntimeError(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orphaned FTIR sample numbers: ['A1439', 'A1904', 'A1910', 'B0076', 'F00129', 'F0054', 'F0057', 'F00742', 'F0156', 'F0225', 'F0272', 'F0285', 'F0343', 'F0378', 'F0571', 'F0795', 'F0876', 'F0883', 'F0967', 'F0983', 'F1179', 'F1197', 'F1201', 'F1277', 'F1413', 'F1518', 'F1878', 'F1880', 'F`1433', 'Z1000']\n",
      "Orphaned Reagent Test sample numbers: ['F0225', 'F0526', 'F0983', 'F1437', 'F1819 (or 1827?)', 'Yellow > green', 'Z1000']\n",
      "Orphaned HR sample numbers: ['F0054', 'F0154', 'F0156', 'F0225', 'F0272', 'F0285', 'F0378', 'F0612', 'F0616', 'F0795', 'F0876', 'F0883', 'F0967', 'F1076', 'F1179', 'F1201', 'F1880', 'F2901', 'F9999', 'G0024', 'G0037', 'G0153', 'G0242', 'G0420', 'G0652', 'G0811', 'G0875', 'G0877', 'G0878', 'G0981', 'G0983', 'G0999', 'G1216', 'G1228', 'G1229', 'G1284', 'G1312', 'G1398', 'G1441', 'G1572', 'G1686', 'G1699', 'G1703', 'G1833', 'G1859', 'G1860']\n",
      "Orphaned MLA sample numbers: ['F0156', 'F0795']\n",
      "Sample numbers only in catalog: ['F0005', 'F0024', 'F0037', 'F0042', 'F0049', 'F0056', 'F0067', 'F0070', 'F0072', 'F0081', 'F0084', 'F0088', 'F0090', 'F0094', 'F0098', 'F0103', 'F0125', 'F0137', 'F0141', 'F0153', 'F0163', 'F0164', 'F0167', 'F0198', 'F0215', 'F0216', 'F0223', 'F0229', 'F0242', 'F0248', 'F0251', 'F0252', 'F0257', 'F0258', 'F0259', 'F0263', 'F0268', 'F0270', 'F0282', 'F0287', 'F0297', 'F0301', 'F0309', 'F0312', 'F0313', 'F0318', 'F0321', 'F0322', 'F0330', 'F0332', 'F0333', 'F0335', 'F0353', 'F0354', 'F0356', 'F0357', 'F0359', 'F0362', 'F0375', 'F0379', 'F0382', 'F0385', 'F0397', 'F0403', 'F0404', 'F0407', 'F0409', 'F0410', 'F0411', 'F0412', 'F0413', 'F0416', 'F0417', 'F0419', 'F0420', 'F0422', 'F0427', 'F0429', 'F0430', 'F0434', 'F0453', 'F0455', 'F0462', 'F0463', 'F0464', 'F0466', 'F0476', 'F0481', 'F0484', 'F0486', 'F0499', 'F0506', 'F0508', 'F0509', 'F0516', 'F0517', 'F0524', 'F0539', 'F0546', 'F0548', 'F0549', 'F0558', 'F0559', 'F0564', 'F0566', 'F0567', 'F0599', 'F0604', 'F0606', 'F0607', 'F0611', 'F0622', 'F0623', 'F0624', 'F0625', 'F0626', 'F0646', 'F0647', 'F0651', 'F0656', 'F0657', 'F0659', 'F0661', 'F0662', 'F0665', 'F0669', 'F0670', 'F0671', 'F0673', 'F0674', 'F0693', 'F0697', 'F0700', 'F0710', 'F0727', 'F0734', 'F0739', 'F0741', 'F0743', 'F0744', 'F0745', 'F0754', 'F0757', 'F0760', 'F0764', 'F0765', 'F0767', 'F0769', 'F0772', 'F0776', 'F0778', 'F0783', 'F0789', 'F0798', 'F0800', 'F0801', 'F0805', 'F0808', 'F0810', 'F0816', 'F0817', 'F0830', 'F0836', 'F0838', 'F0842', 'F0849', 'F0853', 'F0857', 'F0858', 'F0859', 'F0861', 'F0862', 'F0870', 'F0877', 'F0878', 'F0884', 'F0885', 'F0887', 'F0894', 'F0898', 'F0899', 'F0906', 'F0908', 'F0911', 'F0912', 'F0922', 'F0924', 'F0925', 'F0933', 'F0936', 'F0937', 'F0939', 'F0942', 'F0944', 'F0945', 'F0948', 'F0949', 'F0950', 'F0952', 'F0956', 'F0959', 'F0972', 'F0973', 'F0974', 'F0981', 'F1011', 'F1012', 'F1103', 'F1104', 'F1105', 'F1106', 'F1108', 'F1109', 'F1116', 'F1122', 'F1123', 'F1126', 'F1128', 'F1129', 'F1130', 'F1135', 'F1136', 'F1137', 'F1138', 'F1139', 'F1142', 'F1144', 'F1150', 'F1153', 'F1154', 'F1155', 'F1156', 'F1157', 'F1158', 'F1159', 'F1160', 'F1161', 'F1162', 'F1166', 'F1167', 'F1168', 'F1170', 'F1173', 'F1175', 'F1176', 'F1177', 'F1178', 'F1192', 'F1193', 'F1196', 'F1199', 'F1210', 'F1228', 'F1229', 'F1233', 'F1236', 'F1238', 'F1239', 'F1240', 'F1243', 'F1247', 'F1253', 'F1257', 'F1259', 'F1262', 'F1267', 'F1269', 'F1271', 'F1273', 'F1274', 'F1276', 'F1284', 'F1286', 'F1290', 'F1292', 'F1293', 'F1294', 'F1296', 'F1298', 'F1302', 'F1306', 'F1307', 'F1308', 'F1312', 'F1313', 'F1317', 'F1329', 'F1330', 'F1335', 'F1337', 'F1338', 'F1339', 'F1340', 'F1341', 'F1356', 'F1362', 'F1368', 'F1369', 'F1381', 'F1385', 'F1390', 'F1398', 'F1399', 'F1400', 'F1401', 'F1404', 'F1405', 'F1406', 'F1408', 'F1411', 'F1412', 'F1414', 'F1417', 'F1419', 'F1421', 'F1422', 'F1427', 'F1429', 'F1431', 'F1434', 'F1438', 'F1439', 'F1441', 'F1454', 'F1457', 'F1465', 'F1470', 'F1473', 'F1474', 'F1476', 'F1477', 'F1481', 'F1482', 'F1483', 'F1485', 'F1488', 'F1490', 'F1491', 'F1493', 'F1494', 'F1498', 'F1499', 'F1505', 'F1507', 'F1510', 'F1512', 'F1519', 'F1522', 'F1523', 'F1524', 'F1527', 'F1530', 'F1533', 'F1535', 'F1538', 'F1544', 'F1545', 'F1551', 'F1555', 'F1556', 'F1560', 'F1562', 'F1571', 'F1572', 'F1574', 'F1577', 'F1580', 'F1581', 'F1584', 'F1585', 'F1588', 'F1589', 'F1590', 'F1592', 'F1600', 'F1602', 'F1606', 'F1608', 'F1609', 'F1611', 'F1612', 'F1613', 'F1616', 'F1621', 'F1622', 'F1623', 'F1624', 'F1627', 'F1628', 'F1636', 'F1640', 'F1641', 'F1643', 'F1651', 'F1652', 'F1653', 'F1656', 'F1660', 'F1665', 'F1666', 'F1668', 'F1669', 'F1672', 'F1674', 'F1675', 'F1680', 'F1682', 'F1683', 'F1686', 'F1689', 'F1699', 'F1701', 'F1704', 'F1706', 'F1713', 'F1714', 'F1716', 'F1722', 'F1723', 'F1725', 'F1726', 'F1732', 'F1733', 'F1736', 'F1741', 'F1744', 'F1751', 'F1752', 'F1753', 'F1754', 'F1757', 'F1761', 'F1769', 'F1774', 'F1785', 'F1791', 'F1801', 'F1805', 'F1808', 'F1809', 'F1810', 'F1812', 'F1817', 'F1820', 'F1825', 'F1845', 'F1848', 'F1851', 'F1860', 'F1866', 'F1867', 'F1871', 'F1877', 'F1887', 'F1893', 'F1896', 'F1901', 'F1904', 'F1905', 'F1908', 'F1909', 'F1910', 'F1912', 'F1913', 'F1914', 'F1915', 'R0876', 'W1168', 'W1464']\n",
      "Samples not in FTIR or Reagent: ['F0368', 'F0369', 'F0871', 'F0879', 'F1582', 'F1746', 'F1819']\n",
      "### Please fix orphaned/catalog only samples ###\n"
     ]
    }
   ],
   "source": [
    "# Check there are no sample numbers in any of the other spreadsheets that aren't in the cataolog sheet\n",
    "catalog_unique = set(dfs.catalog['Sample Number'].unique())\n",
    "\n",
    "ftir_unique = set(dfs.ftir['Sample Number'].unique())\n",
    "ftir_orphan = ftir_unique.difference(catalog_unique)\n",
    "if ftir_orphan:\n",
    "    print(\"Orphaned FTIR sample numbers: %s\" % sorted(ftir_orphan))\n",
    "\n",
    "reagent_unique = set(dfs.reagent['Sample Number'].unique())\n",
    "reagent_orphan = reagent_unique.difference(catalog_unique)\n",
    "if reagent_orphan:\n",
    "    print(\"Orphaned Reagent Test sample numbers: %s\" % sorted(reagent_orphan))\n",
    "\n",
    "hr_unique = set(dfs.hr['Sample Number'].unique())\n",
    "hr_orphan = hr_unique.difference(catalog_unique)\n",
    "if hr_orphan:\n",
    "    print(\"Orphaned HR sample numbers: %s\" % sorted(hr_orphan))\n",
    "    \n",
    "mla_unique = set(dfs.mla['Sample Number'].unique()).difference(catalog_unique)\n",
    "mla_orphan = mla_unique.difference(catalog_unique)\n",
    "if mla_orphan:\n",
    "    print(\"Orphaned MLA sample numbers: %s\" % sorted(mla_orphan))\n",
    "    \n",
    "# Check for any that are only in the catalog\n",
    "outside_catalog = set.union(reagent_unique, hr_unique, mla_unique)\n",
    "catalog_only = catalog_unique.difference(outside_catalog)\n",
    "if catalog_only:\n",
    "    print(\"Sample numbers only in catalog: %s\" % sorted(catalog_only))\n",
    "    \n",
    "# Check for any that aren't in FTIR and don't have anything in reagent test\n",
    "ftir_missing = catalog_unique.difference(ftir_unique).difference(reagent_unique).difference(catalog_only)\n",
    "if len(ftir_missing):\n",
    "    print(\"Samples not in FTIR or Reagent: %s\" % sorted(ftir_missing))\n",
    "\n",
    "all_unique = copy.copy(ftir_unique)\n",
    "all_unique.update(reagent_unique, hr_unique, mla_unique)\n",
    "if (all_unique or catalog_only):\n",
    "    outs = \"### Please fix orphaned/catalog only samples ###\"\n",
    "    print(outs)\n",
    "    #raise RuntimeError(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up catalog\n",
    "# Drop all unwanted columns\n",
    "\n",
    "#  or 'Your initials'\n",
    "l = ['Which device was a photo taken with? Who does it belong to?',\n",
    "     'Is a breakline present?',\n",
    "     'Unusual appearance'\n",
    "    ]\n",
    "i1 = 'Your initials'\n",
    "i2 = 'Your name and first initial'\n",
    "if i1 in dfs.catalog.columns:\n",
    "    l.append(i1)\n",
    "elif i2 in dfs.catalog.columns:\n",
    "    l.append(i2)\n",
    "dfs.catalog.drop(l, axis=1, inplace=True)\n",
    "\n",
    "d = {\n",
    "    'Timestamp' : 'Catalog timestamp',\n",
    "    'Sample Advertised/Acquired/Sold As': 'Catalog Sold As',\n",
    "    'Sample Form' : 'Catalog form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'Catalog tried',\n",
    "    'What is the mass? (mg)': 'Full pill mass',\n",
    "    'What is the shape of the pill?': 'Pill shape',\n",
    "    'What is the logo?': 'Pill logo',\n",
    "    'What colour is the pill?': 'Pill colour'\n",
    "}\n",
    "dfs.catalog.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must specify axis=0 or 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-aa387cea21f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 'Substance detected', 'Hit Confidence' column where the substance detected was 'other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Copy values from 'Compound detected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hit Confidence.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Brief Note'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jmht/miniconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(self, cond, other, inplace, axis, level, try_cast, raise_on_error)\u001b[0m\n\u001b[1;32m   5338\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5339\u001b[0m         return self._where(cond, other, inplace, axis, level, try_cast,\n\u001b[0;32m-> 5340\u001b[0;31m                            raise_on_error)\n\u001b[0m\u001b[1;32m   5341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5342\u001b[0m     @Appender(_shared_docs['where'] % dict(_shared_doc_kwargs, cond=\"False\",\n",
      "\u001b[0;32m/Users/jmht/miniconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_where\u001b[0;34m(self, cond, other, inplace, axis, level, try_cast, raise_on_error)\u001b[0m\n\u001b[1;32m   5109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5110\u001b[0m                 _, other = self.align(other, join='left', axis=axis,\n\u001b[0;32m-> 5111\u001b[0;31m                                       level=level, fill_value=np.nan)\n\u001b[0m\u001b[1;32m   5112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5113\u001b[0m                 \u001b[0;31m# if we are NOT aligned, raise as we cannot where index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jmht/miniconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36malign\u001b[0;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\u001b[0m\n\u001b[1;32m   2726\u001b[0m                                             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m                                             \u001b[0mfill_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m                                             broadcast_axis=broadcast_axis)\n\u001b[0m\u001b[1;32m   2729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reindex'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jmht/miniconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36malign\u001b[0;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\u001b[0m\n\u001b[1;32m   4935\u001b[0m                                       \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4936\u001b[0m                                       \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4937\u001b[0;31m                                       fill_axis=fill_axis)\n\u001b[0m\u001b[1;32m   4938\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4939\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unsupported type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jmht/miniconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_align_series\u001b[0;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)\u001b[0m\n\u001b[1;32m   5034\u001b[0m                     \u001b[0mfdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5035\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5036\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must specify axis=0 or 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5038\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must specify axis=0 or 1"
     ]
    }
   ],
   "source": [
    "# For FTIR columns need to merge the data from the 'Compound detected', 'Hit Confidence.1' columns into the\n",
    "# 'Substance detected', 'Hit Confidence' column where the substance detected was 'other'\n",
    "mask = dfs.ftir['Substance detected'] != 'Other'\n",
    "dfs.ftir['Substance detected'].where(mask, dfs.ftir['Compound detected'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence'].where(mask, dfs.ftir['Hit Confidence.1'], inplace=True)\n",
    "dfs.ftir.drop(['Compound detected', 'Hit Confidence.1', 'Brief Note'], axis=1, inplace=True)\n",
    "\n",
    "mask = dfs.ftir['Compound detected (Subtraction)'] != 'Other'\n",
    "dfs.ftir['Compound detected (Subtraction)'].where(mask, df_ftir['Substance detected.1'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence.2'].where(mask, dfs.ftir['Hit Confidence.3'], inplace=True)\n",
    "dfs.ftir.drop(['Substance detected.1', 'Hit Confidence.3', 'Brief Note.1'], axis=1, inplace=True)\n",
    "\n",
    "# Drop all unwanted columns\n",
    "l = ['Your name and surname initial',\n",
    "     'User Suspicion',\n",
    "     'Is anything detected after subtraction analysis?',\n",
    "     'Analysis required',\n",
    "     'Note for harm reduction worker'\n",
    "    ]\n",
    "dfs.ftir.drop(l, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'FTIR timestamp',\n",
    "    'Sample Sold As': 'FTIR Sold As',\n",
    "    'Sample Form' : 'FTIR form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'FTIR tried',\n",
    "    'Substance(s) detected' : 'FTIR final result',\n",
    "    'Substance detected' : 'FTIR result1',\n",
    "    'Hit Confidence' :  'FTIR hit1',\n",
    "    'Is anything detected after subtraction analysis?' : 'FTIR subtraction positive',\n",
    "    'Compound detected (Subtraction)' :  'FTIR result2',\n",
    "    'Hit Confidence.2' :  'FTIR hit2',\n",
    "    '\"Strength\" of powdered substance' : 'FTIR Powder Strength',\n",
    "    'Does the substance detected match the substance that was advertised?' : 'FTIR Matches Sold As',\n",
    "}\n",
    "dfs.ftir.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up HR form\n",
    "\n",
    "# Drop all unwanted columns\n",
    "l = ['HR worker name:']\n",
    "dfs.hr.drop(l, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'HR timestamp',\n",
    "    'You submitted a substance for analysis. What were you told it was when you got it?': 'HR Sold as',\n",
    "    'Had you already tried this substance before getting it tested?': 'HR tried',\n",
    "    'What was your first sample number at this event? Did you take a photo or keep the ticket?': 'Previous Sample Number'\n",
    "}\n",
    "dfs.hr.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Catalog and FTIR data frames\n",
    "df_all = pd.merge(dfs.catalog, dfs.ftir, how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in any reagent test data\n",
    "df_all = pd.merge(df_all, dfs.reagent[['Sample Number', 'Reagent Result']], how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in any pill strength data\n",
    "df_all = pd.merge(df_all, dfs.mla[['Sample Number', 'MDMA / tablet (mg)', '% MDMA content']], how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in HR data\n",
    "df_all = pd.merge(df_all, dfs.hr, how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix column orders\n",
    "prefix = ['Sample Number',\n",
    "          'Catalog timestamp', 'FTIR timestamp', 'HR timestamp',\n",
    "          'Catalog Sold As', 'FTIR Sold As','HR Sold as', \n",
    "          'Catalog form', 'FTIR form',\n",
    "          'Catalog tried', 'FTIR tried', 'HR tried']\n",
    "columns = [c for c in df_all.columns if c not in prefix]\n",
    "columns = prefix + columns\n",
    "df_all = df_all[columns]\n",
    "df_all.to_csv('foo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
